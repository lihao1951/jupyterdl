{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model中训练和评估的函数\n",
    "\n",
    "fit:训练\n",
    "\n",
    "evaluate：评估\n",
    "\n",
    "predict：预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样例如下\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换数据，并划分训练、验证、测试集合\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程配置\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-4),loss=keras.losses.SparseCategoricalCrossentropy(),metrics=[keras.metrics.SparseCategoricalAccuracy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "50000/50000 [==============================] - 3s 56us/sample - loss: 0.9824 - sparse_categorical_accuracy: 0.7394 - val_loss: 0.3978 - val_sparse_categorical_accuracy: 0.8978\n",
      "Epoch 2/2\n",
      "50000/50000 [==============================] - 2s 33us/sample - loss: 0.3661 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.2913 - val_sparse_categorical_accuracy: 0.9170\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "# 定义epoch和batch-size\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=2,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.9823776105308533, 0.3661482168340683],\n",
       " 'sparse_categorical_accuracy': [0.73936, 0.90004],\n",
       " 'val_loss': [0.3978313138961792, 0.29129388489723207],\n",
       " 'val_sparse_categorical_accuracy': [0.8978, 0.917]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "10000/1 - 0s - loss: 0.2285 - sparse_categorical_accuracy: 0.9174\n",
      "test loss, test acc: [0.2982235023736954, 0.9174]\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=64,verbose=2)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于compile中许多内置的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer\n",
    "    - SGD()\n",
    "    - RMSprop()\n",
    "    - Adam()\n",
    "    - etc.\n",
    "- Losses:\n",
    "    - MeanSquaredError()\n",
    "    - KLDivergence()\n",
    "    - CosineSimilarity()\n",
    "    - etc.\n",
    "- Metrics:\n",
    "    - AUC()\n",
    "    - Precision()\n",
    "    - Recall()\n",
    "    - etc.\n",
    "    \n",
    "如果要自定义loss，需要继承tf.keras.losses.Loss类，此时需要重写__init__ and call(self,y_true,y_pred)\n",
    "\n",
    "示例如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSE(keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super().__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse + reg * self.regularization_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要自定义metrics，需要继承tf.keras.metrics.Metric，需要实现4中方法：\n",
    "- __init__(self):为了你的metric目标创建中间参数\n",
    "- update_state(self, y_true, y_pred, sample_weight=None)：利用真实值和预测值更新中间值\n",
    "- result(self):利用中间值得出最终结果\n",
    "- reset_states(self):重新初始化中间状态\n",
    "\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTruePositives(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 2s 37us/sample - loss: 0.3393 - categorical_true_positives: 45154.0000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 28us/sample - loss: 0.1598 - categorical_true_positives: 47623.0000\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 28us/sample - loss: 0.1163 - categorical_true_positives: 48272.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23814d447b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[CategoricalTruePositives()],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 37us/sample - loss: 2.4918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23819d1fc88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加正则化损失\n",
    "class ActivityRegulariationLayer(keras.layers.Layer):\n",
    "    def call(self,inputs):\n",
    "        self.add_loss(tf.math.reduce_sum(inputs) * 0.1)\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,),name='digits')\n",
    "x=layers.Dense(64,activation='relu',name='dense_1')(inputs)\n",
    "x=ActivityRegulariationLayer()(x)\n",
    "x = layers.Dense(64,activation='relu',name='dense_2')(x)\n",
    "outputs = layers.Dense(10,name='pred')(x)\n",
    "model=keras.Model(inputs=inputs,outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以添加对评估函数的日志记录-利用add_metric()方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 35us/sample - loss: 0.3415 - std_of_activation: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2381a6c9be0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self,inputs):\n",
    "        self.add_metric(keras.backend.std(inputs),name='std_of_activation',aggregation='mean')\n",
    "        return inputs\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 40us/sample - loss: 2.5361 - std_of_activation: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2381987f710>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在创建好模型之后 也可以利用模型model的方法 add_loss add_metric\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "40000/40000 [==============================] - 2s 52us/sample - loss: 0.3632 - sparse_categorical_accuracy: 0.8988 - val_loss: 0.2199 - val_sparse_categorical_accuracy: 0.9334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2381c7815f8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用验证集\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4031 - sparse_categorical_accuracy: 0.8893\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1971 - sparse_categorical_accuracy: 0.9434\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1470 - sparse_categorical_accuracy: 0.9570\n",
      "Evaluate\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.1388 - sparse_categorical_accuracy: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.13879615733329254, 'sparse_categorical_accuracy': 0.9602}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(128)\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# You can also evaluate or predict on a dataset.\n",
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用dataset的验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3984 - sparse_categorical_accuracy: 0.8893 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1884 - sparse_categorical_accuracy: 0.9448 - val_loss: 0.1576 - val_sparse_categorical_accuracy: 0.9547\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 0.1397 - sparse_categorical_accuracy: 0.9582 - val_loss: 0.1359 - val_sparse_categorical_accuracy: 0.9579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2381cfed4a8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持其他的输入格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前常用的输入格式有\n",
    "- Numpy\n",
    "- Dataset\n",
    "- Sequence\n",
    "\n",
    "前两个上面已经试验过，而利用Sequence，需要继承类 keras.util.Sequence，且必须实现 __getitem__ 、__len__方法，__getitem__ 方法必须返回一个完整的batch，如果你想在每次epoch之间改变数据，那么你还需要实现on_epoch_end 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于Sequence的实践\n",
    "class CIFAR10Sequence(keras.utils.Sequence):\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        self.filenames, self.labels = filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回总batch数目\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 利用idx值来作为step数，得到一个batch的 x,y\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array([\n",
    "            resize(imread(filename), (200, 200))\n",
    "               for filename in batch_x]), np.array(batch_y)\n",
    "# 和Dataset类似的使用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用样本权重和类别权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，数据集中的样本权重由他的频数来确定，有两种方法来对数据权重进行赋值\n",
    "- 类别权重\n",
    "- 样本权重\n",
    "\n",
    "### 类别权重\n",
    "在fit()的时候传入 class_weight ，这是一个dict格式，定义每个类别的权重值，类型为double。一般来说对于数量较少的类赋予较大的权重，有助于缓解样本不平衡训练,例如如下格式\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class_weight = {\n",
    "    0: 1.0,\n",
    "    1: 1.0,\n",
    "    2: 1.0,\n",
    "    3: 1.0,\n",
    "    4: 1.0,\n",
    "    # Set weight \"2\" for class \"5\",\n",
    "    # making this class 2x more important\n",
    "    5: 2.0,\n",
    "    6: 1.0,\n",
    "    7: 1.0,\n",
    "    8: 1.0,\n",
    "    9: 1.0,\n",
    "}\n",
    "\n",
    "print(\"Fit with class weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本权重\n",
    "当没建立分类模型，可以使用样本权重，此时对于tf.keras而言，有两种情况：\n",
    "- 输入数据为Numpy，此时fit()使用sample_weight参数 \n",
    "- 输入数据为Dataset或者其他迭代器类型：Yield（input_batch,label_batch,sample_weight_batch） tuples\n",
    "样本权重一般为数组形式，指定每个数据在batch的权重值，同样用在样本不平衡的场景下，少数量的样本，权重更大。示例如下代码\n",
    "\n",
    "```python\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "print(\"Fit with sample weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1)\n",
    "```\n",
    "下面是对于Dataset的版本\n",
    "\n",
    "```python\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多输入和多输出的模型创建\n",
    "以上的模型均为单输入和单输出的模型，对于多输入和输出模型也类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个输入\n",
    "image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
    "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "# 两个输出\n",
    "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
    "class_output = layers.Dense(5, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[image_input, timeseries_input], outputs=[score_output, class_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output score_output missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to score_output.\n"
     ]
    }
   ],
   "source": [
    "# 在complie种需要注意，有多种方法可以进行设置\n",
    "# 1.loss按照数组设置，分别对应添加的两个不同的输出，如果指定一个loss，那么对于不同输出会应用一样的loss函数\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),loss=[keras.losses.MeanSquaredError(),keras.losses.CategoricalCrossentropy()])\n",
    "# 2.对于metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss=[keras.losses.MeanSquaredError(),keras.losses.CategoricalCrossentropy()],\n",
    "    metrics=[\n",
    "        [keras.metrics.MeanAbsolutePercentageError(),\n",
    "         keras.metrics.MeanAbsoluteError()\n",
    "        ],\n",
    "        [keras.metrics.CategoricalAccuracy()]\n",
    "    ]\n",
    ")\n",
    "# 3.类似的  我们还可以指定名称，利用[dict]这种方式来设置\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss={\n",
    "        'score_output':keras.losses.MeanSquaredError(),\n",
    "        'class_output':keras.losses.CategoricalCrossentropy()\n",
    "    },\n",
    "    metrics={\n",
    "        'score_output':[\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError()\n",
    "        ],\n",
    "        'class_output':[keras.metrics.CategoricalAccuracy()]\n",
    "    }\n",
    ")\n",
    "# 4.同样也可以对不同的输出损失进行权重指定\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy()\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()]\n",
    "    },\n",
    "    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
    ")\n",
    "# 5.也可以不选择损失函数 ，仅把输出结果显示不用来训练\n",
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "100/100 [==============================] - 2s 16ms/sample - loss: 6.7461 - score_output_loss: 2.1003 - class_output_loss: 4.9883\n",
      "Train on 100 samples\n",
      "100/100 [==============================] - 0s 366us/sample - loss: 5.8471 - score_output_loss: 1.2582 - class_output_loss: 4.6432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23821e090b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在训练过程中也是要指定名称来对输入输出数据集指定\n",
    "import numpy as np\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# Generate dummy NumPy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit(\n",
    "    {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "    {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 255ms/step - loss: 5.4599 - score_output_loss: 0.9415 - class_output_loss: 4.5185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23821dc6b00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当传入的数据为Dataset格式，那么需要在生成Dataset时进行名称指定\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "        {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用callbacks\n",
    "callbakcs一般在keras种用来为不同的训练目标来设定的，一般有以下实现的功能\n",
    "1. 在训练中 进行可视化\n",
    "2. checkpoint\n",
    "3. 改变学习速率\n",
    "4. 在训练中对层做fune-tuning\n",
    "5. 发送Email或其他一些信息\n",
    "6. etc.\n",
    "\n",
    "callbacks 一般在fit()使用，例如如下 常用的早停机制\n",
    "\n",
    "```python\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写自己的callback\n",
    "需要继承 keras.callbacks.Callback类，在fit、evaluate、predict种均可使用，在继承类之后按需要实现以下方法，实现callback\n",
    "- on_(train|test|predict)_begin(self, logs=None) 在训练/验证/测试 开始时\n",
    "- on_(train|test|predict)_end(self, logs=None) 在训练/验证/测试 结束后\n",
    "- on_(train|test|predict)_batch_begin(self, batch, logs=None) 在训练/验证/测试 batch开始\n",
    "- on_(train|test|predict)_batch_end(self, batch, logs=None) 在训练/验证/测试 batch结束\n",
    "- on_epoch_begin(self, epoch, logs=None) 在每一个epoch开始 仅train生效\n",
    "- on_epoch_end(self, epoch, logs=None)  在每一个epoch结束 仅train生效\n",
    "\n",
    "其中logs是一个dict，里面包含了每个步骤下的loss，step中为trainloss，在epoch结束时有loss 和设定的其他指标入 metrics等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "当使用大量数据进行模型训练时，应该在一定的频次间隔保存训练好的模型，最简单的就是利用 keras.callbacks.ModelCheckpoint 来设定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "38464/40000 [===========================>..] - ETA: 0s - loss: 0.3804 - sparse_categorical_accuracy: 0.8928\n",
      "Epoch 00001: val_loss improved from inf to 0.23963, saving model to mymodel_1\n",
      "WARNING:tensorflow:From D:\\applications\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: mymodel_1\\assets\n",
      "40000/40000 [==============================] - 3s 63us/sample - loss: 0.3741 - sparse_categorical_accuracy: 0.8946 - val_loss: 0.2396 - val_sparse_categorical_accuracy: 0.9270\n",
      "Epoch 2/2\n",
      "39872/40000 [============================>.] - ETA: 0s - loss: 0.1778 - sparse_categorical_accuracy: 0.9480\n",
      "Epoch 00002: val_loss improved from 0.23963 to 0.20471, saving model to mymodel_2\n",
      "INFO:tensorflow:Assets written to: mymodel_2\\assets\n",
      "40000/40000 [==============================] - 2s 48us/sample - loss: 0.1778 - sparse_categorical_accuracy: 0.9480 - val_loss: 0.2047 - val_sparse_categorical_accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2382fe92e80>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"mymodel_{epoch}\",\n",
    "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在训练中保存模型并在停止时恢复上次训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Prepare a directory to store all the checkpoints.\n",
    "checkpoint_dir = \".\\\\ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "def make_or_restore_model():\n",
    "    # Either restore the latest model, or create a fresh one\n",
    "    # if there is no checkpoint available.\n",
    "    checkpoints = [checkpoint_dir + \"\\\\\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print(\"Restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    print(\"Creating a new model\")\n",
    "    return get_compiled_model()\n",
    "\n",
    "\n",
    "model = make_or_restore_model()\n",
    "callbacks = [\n",
    "    # This callback saves a SavedModel every 100 batches.\n",
    "    # We include the training loss in the saved model name.\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + \"\\\\ckpt-loss={loss:.2f}\", save_freq=100\n",
    "    )\n",
    "]\n",
    "model.fit(x_train, y_train, epochs=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运用学习率schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,decay_steps=100000,decay_rate=0.96,staircase=True\n",
    ")\n",
    "# schedules 包括了 ExponentialDecay、PiecewiseConstantDecay、PolynomialDecay、InverseTimeDecay\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种方式运行\n",
    "第一种：按照callback来做\n",
    "第二种：自己编写每一步train_step,在tf.GradientTape()外调用，记录每一步的loss或者其他指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 6s 122us/sample - loss: 0.1457 - sparse_categorical_accuracy: 0.9569 - val_loss: 0.1188 - val_sparse_categorical_accuracy: 0.9653\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 0.1072 - sparse_categorical_accuracy: 0.9688 - val_loss: 0.1188 - val_sparse_categorical_accuracy: 0.9647\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 0.0887 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.1086 - val_sparse_categorical_accuracy: 0.9706\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 6s 116us/sample - loss: 0.0763 - sparse_categorical_accuracy: 0.9784 - val_loss: 0.1102 - val_sparse_categorical_accuracy: 0.9716\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 0.0661 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.1194 - val_sparse_categorical_accuracy: 0.9694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2389b207080>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一种\n",
    "log_dir = '.\\\\logs\\\\fit\\\\'\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
    "\n",
    "def train_step(model, optimizer, x_train, y_train):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(x_train, training=True)\n",
    "    loss = loss_object(y_train, predictions)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y_train, predictions)\n",
    "\n",
    "def test_step(model, x_test, y_test):\n",
    "  predictions = model(x_test)\n",
    "  loss = loss_object(y_test, predictions)\n",
    "\n",
    "  test_loss(loss)\n",
    "  test_accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.268586665391922, Accuracy: 92.25399780273438, Test Loss: 0.1262025088071823, Test Accuracy: 96.31999969482422\n",
      "Epoch 2, Loss: 0.11531289666891098, Accuracy: 96.552001953125, Test Loss: 0.09362193197011948, Test Accuracy: 97.23999786376953\n",
      "Epoch 3, Loss: 0.07867169380187988, Accuracy: 97.58399963378906, Test Loss: 0.0755162239074707, Test Accuracy: 97.61000061035156\n",
      "Epoch 4, Loss: 0.058402884751558304, Accuracy: 98.21599578857422, Test Loss: 0.07108774036169052, Test Accuracy: 97.86000061035156\n",
      "Epoch 5, Loss: 0.045410677790641785, Accuracy: 98.51599884033203, Test Loss: 0.07189565151929855, Test Accuracy: 97.81999969482422\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "model = create_model() # reset our model\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for (x_train, y_train) in train_dataset:\n",
    "    train_step(model, optimizer, x_train, y_train)\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "  for (x_test, y_test) in test_dataset:\n",
    "    test_step(model, x_test, y_test)\n",
    "  with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "  \n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "  # Reset metrics every epoch\n",
    "  train_loss.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
